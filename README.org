#+TITLE: Readme

# This program is to create an inverted index in an I/O efficient manner, given ~20GB document file.
* Index construction in three phase: parsing phase, merging phase, and index building phase
** Parsing phase includes sequentially parse .trec file
*** for each document, we track (term_ID, doc_ID, freq) tuples in a buffer and later saved to a intermediate file format to perform k-way merge.
*** we also using a dictionary to keep doc_ID to URL mapping in memory, also the mappings between actual term and term_ID, these are saved in common serialization format for portability
** Merging phase includes merge sorting all intermediate file in previous phase to a single file
*** each intermediate file can be pre-sorted in memory before dumped to disk, so we just perform external k-way merge
*** By using a self defined intermediate file format in previous step, we can customize partial read functionality of a large file
**** We used `CachedFile` to its `forward` method to abstract the partial read behavior
** Index building phase
*** building inverted index by scan through the merged file in merging phase, and build inverted list one by one, and save as a similiar format as page_table, term_ID_to_term mapping, term_ID_to_term mapping, lexicon
**** why batched compression talked in piazza/slides seems still questionable to me and left unimplemented: By experimenting, I observe that posting tuple has a long-tail distribution, which means there's so many word(may depends on tokenization) that only appears once, and using block to push postings of a single term together introduce overhead of metadata, the number of infrequent words will yields wasted space of metadata. If we consider another tokenization method that may benefits from, then we are introducing improvements that are tied to overall feature completeness and performance of the whole system, which is unfavorable to me.
